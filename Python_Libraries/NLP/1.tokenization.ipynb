{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "069bfed7-f97a-4c15-96ae-a30ca2cb395d",
   "metadata": {},
   "source": [
    "# NLTK - Natural Language ToolKit\n",
    "\n",
    "## Important Points about NLTK (Natural Language Toolkit)\n",
    "\n",
    "1. **What It Is**  \n",
    "   NLTK is a powerful Python library for **natural language processing (NLP)** and **text analysis**. It helps in processing and analyzing human language data.\n",
    "\n",
    "2. **Open Source**  \n",
    "   It is open-source and freely available, widely used in academic and research settings.\n",
    "\n",
    "3. **Wide Range of Tools**  \n",
    "   NLTK provides functionalities for:\n",
    "   - Tokenization (splitting text into words/sentences)\n",
    "   - Stemming and Lemmatization\n",
    "   - POS (Part-of-Speech) tagging\n",
    "   - Named Entity Recognition (NER)\n",
    "   - Parsing and Syntax Trees\n",
    "   - Basic Sentiment Analysis\n",
    "\n",
    "4. **Corpora and Lexical Resources**  \n",
    "   Includes a variety of **built-in corpora** (e.g., Gutenberg, Brown, WordNet) for training and testing NLP models.\n",
    "\n",
    "5. **Integration**  \n",
    "   Works well with other Python libraries like:\n",
    "   - `scikit-learn` (machine learning)\n",
    "   - `NumPy` (numerical computing)\n",
    "   - `matplotlib` (visualization)\n",
    "\n",
    "6. **Good for Prototyping**  \n",
    "   Ideal for building prototypes and learning NLP concepts. However, it may be **slower** than other libraries like `spaCy` in production environments.\n",
    "\n",
    "7. **Language Support**  \n",
    "   Primarily supports **English**, though it can be adapted for other languages with some limitations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb1d75-53d3-42e4-abaa-0afbf7688a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c042de47-3492-4414-8e79-51f232d59df3",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "### What is Tokenization?\n",
    "\n",
    "**Tokenization** is the process of breaking a text into smaller units called **tokens**. These tokens can be:\n",
    "- **Words**\n",
    "- **Sentences**\n",
    "- **Subwords** (depending on the application)\n",
    "\n",
    "Tokenization is often the **first step** in Natural Language Processing (NLP), as it prepares raw text for further analysis.\n",
    "\n",
    "## Types of Tokenization\n",
    "\n",
    "1. **Word Tokenization**  \n",
    "   Splits text into individual words or terms.\n",
    "   - Example:  \n",
    "     `\"Hello world!\"` → `['Hello', 'world', '!']`\n",
    "\n",
    "2. **Sentence Tokenization**  \n",
    "   Splits a paragraph into sentences.\n",
    "   - Example:  \n",
    "     `\"Hello world! How are you?\"` → `['Hello world!', 'How are you?']`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c55c56c1-beac-4790-b204-0ca026be6201",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_corpus = \"\"\"Hello there! How are you doing today?\n",
    "I hope you're enjoying learning about Natural Language Processing.\n",
    "Let's explore more with NLTK and Python.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae1656af-b121-4d89-bd40-cff0e249ecc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! How are you doing today?\n",
      "I hope you're enjoying learning about Natural Language Processing.\n",
      "Let's explore more with NLTK and Python.\n"
     ]
    }
   ],
   "source": [
    "print(sample_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7031dc0-152b-4195-95e7-98c93956c1f9",
   "metadata": {},
   "source": [
    "### paragraph ---> sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9602735a-617d-436c-b11b-47a0757cd818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5b0f01f-a87a-482b-89fa-845310c313b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = sent_tokenize(sample_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c1b516d-9e53-444e-9e77-409488a2f3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello there!',\n",
       " 'How are you doing today?',\n",
       " \"I hope you're enjoying learning about Natural Language Processing.\",\n",
       " \"Let's explore more with NLTK and Python.\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d53a98ed-2d1c-4aac-b96c-c7b8ef910219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there!\n",
      "How are you doing today?\n",
      "I hope you're enjoying learning about Natural Language Processing.\n",
      "Let's explore more with NLTK and Python.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f4fd1-f956-4fb6-9fd4-213d7730aef4",
   "metadata": {},
   "source": [
    "### paragraph ---> words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e3e84e9-bb21-4f51-815c-0ac863f7ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7e8b2c9-7c43-46b7-b6b5-a15bca474ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(sample_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4272c667-0a18-47f1-bcae-a810a95e167f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'there',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'today',\n",
       " '?',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'you',\n",
       " \"'re\",\n",
       " 'enjoying',\n",
       " 'learning',\n",
       " 'about',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '.',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'explore',\n",
       " 'more',\n",
       " 'with',\n",
       " 'NLTK',\n",
       " 'and',\n",
       " 'Python',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7315ac07-92fd-4a49-8fae-dd63f7793aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "there\n",
      "!\n",
      "How\n",
      "are\n",
      "you\n",
      "doing\n",
      "today\n",
      "?\n",
      "I\n",
      "hope\n",
      "you\n",
      "'re\n",
      "enjoying\n",
      "learning\n",
      "about\n",
      "Natural\n",
      "Language\n",
      "Processing\n",
      ".\n",
      "Let\n",
      "'s\n",
      "explore\n",
      "more\n",
      "with\n",
      "NLTK\n",
      "and\n",
      "Python\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150470a4-465c-44a4-b071-b6c6ce469757",
   "metadata": {},
   "source": [
    "### sentence to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f0fd02a-5a0f-4cdb-ba50-4a9f0c176c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'there', '!']\n",
      "['How', 'are', 'you', 'doing', 'today', '?']\n",
      "['I', 'hope', 'you', \"'re\", 'enjoying', 'learning', 'about', 'Natural', 'Language', 'Processing', '.']\n",
      "['Let', \"'s\", 'explore', 'more', 'with', 'NLTK', 'and', 'Python', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47563b3-b8d7-4085-95cf-0115a54aad04",
   "metadata": {},
   "source": [
    "## `wordpunct_tokenize` in NLTK\n",
    "\n",
    "`wordpunct_tokenize` is a tokenizer in NLTK that **splits text into alphabetic and non-alphabetic characters** using simple regex rules.\n",
    "\n",
    "- It separates **words** and **punctuation** as distinct tokens.\n",
    "- Useful for basic tokenization where punctuation needs to be isolated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70354b12-d0ff-439a-ba79-6240b056a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41ef4ca2-3e8f-48ff-96eb-09bd1ccb642e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'there',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'today',\n",
       " '?',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'you',\n",
       " \"'\",\n",
       " 're',\n",
       " 'enjoying',\n",
       " 'learning',\n",
       " 'about',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '.',\n",
       " 'Let',\n",
       " \"'\",\n",
       " 's',\n",
       " 'explore',\n",
       " 'more',\n",
       " 'with',\n",
       " 'NLTK',\n",
       " 'and',\n",
       " 'Python',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(sample_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3d774d-a2c6-4d2f-be2d-09cd7a9a5f5c",
   "metadata": {},
   "source": [
    "## `TreebankWordTokenizer` in NLTK\n",
    "\n",
    "`TreebankWordTokenizer` is a rule-based tokenizer from NLTK that uses **Penn Treebank** conventions.\n",
    "\n",
    "- It tokenizes text into words and punctuation using specific rules.\n",
    "- Handles **contractions**, **punctuation**, and **quotes** more accurately than basic tokenizers.\n",
    "\n",
    "#### Key Features:\n",
    "- Splits contractions like *“don’t”* into *“do”* and *“n’t”*\n",
    "- Treats punctuation as separate tokens\n",
    "- Handles cases like *“Mr.”*, *“U.S.”*, and parentheses well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41fb6be6-c63e-4f58-8512-028346c4a1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a51ca759-bcc3-4787-8415-24b1d5c9d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e22e4dee-e577-42d2-a6b2-090161450cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'there',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'today',\n",
       " '?',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'you',\n",
       " \"'re\",\n",
       " 'enjoying',\n",
       " 'learning',\n",
       " 'about',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing.',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'explore',\n",
       " 'more',\n",
       " 'with',\n",
       " 'NLTK',\n",
       " 'and',\n",
       " 'Python',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(sample_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac618d00-39b8-4a43-a344-2e1f7a9e66bf",
   "metadata": {},
   "source": [
    "### Comparison of NLTK Tokenizers\n",
    "\n",
    "| Feature                        | `word_tokenize`                     | `wordpunct_tokenize`                   | `TreebankWordTokenizer`               |\n",
    "|-------------------------------|-------------------------------------|----------------------------------------|---------------------------------------|\n",
    "| Based on                      | Penn Treebank + Punkt sentence tokenizer | Simple regex rules                     | Penn Treebank rules                   |\n",
    "| Handles contractions well     | ✅ (e.g., \"don't\" → [\"do\", \"n't\"])   | ❌ (\"don't\" → [\"don\", \"'\", \"t\"])       | ✅ (\"don't\" → [\"do\", \"n't\"])          |\n",
    "| Punctuation as separate token | ✅                                   | ✅                                      | ✅                                     |\n",
    "| Keeps sentence structure      | ✅                                   | ❌                                      | ✅                                     |\n",
    "| Splits on non-alphanumerics  | ❌                                   | ✅ (splits on every non-letter)         | ❌                                     |\n",
    "| Suitable for NLP tasks        | ✅ (default, balanced choice)        | ❌ (too aggressive)                     | ✅ (used in syntactic parsing)        |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01fe648d-4661-4e82-b632-102628934c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize: ['She', 'said', ',', '``', 'Do', \"n't\", 'do', 'it', '!', \"''\"]\n",
      "wordpunct_tokenize: ['She', 'said', ',', '\"', 'Don', \"'\", 't', 'do', 'it', '!\"']\n",
      "TreebankWordTokenizer: ['She', 'said', ',', '``', 'Do', \"n't\", 'do', 'it', '!', \"''\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, TreebankWordTokenizer\n",
    "\n",
    "text = \"She said, \\\"Don't do it!\\\"\"\n",
    "\n",
    "print(\"word_tokenize:\", word_tokenize(text))\n",
    "print(\"wordpunct_tokenize:\", wordpunct_tokenize(text))\n",
    "print(\"TreebankWordTokenizer:\", TreebankWordTokenizer().tokenize(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytryout_venv",
   "language": "python",
   "name": "mytryout_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
